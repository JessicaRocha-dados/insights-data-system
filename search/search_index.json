{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vis\u00e3o Geral do Projeto InsightOS","text":"<p>Nota: Esta \u00e9 a documenta\u00e7\u00e3o t\u00e9cnica completa. Para o sum\u00e1rio executivo visual e os dashboards de resultados finais para gestores (constru\u00eddos em Streamlit), por favor visite o nosso Hub Estrat\u00e9gico de Insights (no Notion).</p> <p>1.1 Introdu\u00e7\u00e3o</p> <ul> <li>Resumo Executivo</li> </ul> <p>Este documento detalha o ciclo de vida completo do projeto InsightOS, uma iniciativa destinada a transformar dados brutos de usu\u00e1rios em intelig\u00eancia de neg\u00f3cios acion\u00e1vel. O principal desafio identificado foi a inefici\u00eancia na aquisi\u00e7\u00e3o de clientes pela plataforma SaaS InsightOS, que n\u00e3o era capaz de diferenciar leads de alto potencial daqueles com baixo potencial.</p> <p>A solu\u00e7\u00e3o desenvolvida consistiu em um sistema de processamento de dados de ponta a ponta, centrado em um modelo de machine learning para Lead Scoring e complementado por uma an\u00e1lise detalhada de Lifetime Value (LTV). O sistema \u00e9 capaz de acompanhar a jornada dos usu\u00e1rios, calcular uma pontua\u00e7\u00e3o representando a probabilidade de convers\u00e3o e, ao final, automatizar esse processo, garantindo efici\u00eancia operacional.</p> <p>O sistema permite \u00e0 InsightOS identificar leads com maior potencial de convers\u00e3o, otimizar os investimentos em marketing e tomar decis\u00f5es estrat\u00e9gicas de produto com base em dados. Essa abordagem transformou a organiza\u00e7\u00e3o, que passou de reativa para proativa e orientada por dados. Os resultados financeiros validaram a solu\u00e7\u00e3o: a an\u00e1lise de Lifetime Value demonstrou que leads originados da campanha google_gestao_agil geram um retorno 2,3 vezes maior (ou 130% superior) em compara\u00e7\u00e3o com outros canais, fornecendo uma recomenda\u00e7\u00e3o clara para priorizar investimentos nesse segmento.</p> <p>O principal objetivo do projeto foi superar a chamada \"cegueira estrat\u00e9gica\" na aquisi\u00e7\u00e3o de clientes. Para isso, definiram-se as seguintes metas de neg\u00f3cios:</p> <p>Prioriza\u00e7\u00e3o de Vendas: Capacitar a equipe comercial a direcionar seus esfor\u00e7os para leads com maior potencial de convers\u00e3o, maximizando a efici\u00eancia nas opera\u00e7\u00f5es de vendas.</p> <p>Otimiza\u00e7\u00e3o do ROI: Relacionar o investimento em marketing, medido pelo Custo de Aquisi\u00e7\u00e3o de Clientes (CAC), \u00e0 receita gerada, representada pelo Lifetime Value (LTV), fornecendo uma base quantitativa para a aloca\u00e7\u00e3o estrat\u00e9gica de or\u00e7amento.</p> <p>Intelig\u00eancia de Produto: Mapear os principais pontos de atrito na jornada dos usu\u00e1rios para identificar oportunidades de melhoria e promover aprimoramentos estrat\u00e9gicos no produto.</p> <p>Para que os objetivos estrat\u00e9gicos fossem alcan\u00e7ados, foram definidas as seguintes metas t\u00e9cnicas:</p> <ol> <li> <p>Construir um pipeline completo de dados, desde a captura de eventos no navegador at\u00e9 o armazenamento seguro e persistente em banco de dados.</p> </li> <li> <p>Implementar um modelo de Lead Scoring para prever a probabilidade de convers\u00e3o de leads.</p> </li> <li> <p>Desenvolver modelos de Lifetime Value (LTV), com base nas metodologias BG/NBD e Gamma-Gamma, para mensurar o valor financeiro de cada segmento de cliente.</p> </li> <li> <p>Automatizar o processo de c\u00e1lculo e pontua\u00e7\u00e3o de novos leads em um ambiente de produ\u00e7\u00e3o na nuvem.</p> </li> </ol> <p>A arquitetura do sistema foi projetada com uma pilha de tecnologias moderna, escal\u00e1vel e baseada em solu\u00e7\u00f5es de c\u00f3digo aberto. A escolha das ferramentas priorizou alta performance, flexibilidade e confiabilidade:</p> <p>FastAPI: Utilizada para a cria\u00e7\u00e3o da API de ingest\u00e3o de dados, destacando-se por sua elevada performance simult\u00e2nea e valida\u00e7\u00e3o de dados nativa atrav\u00e9s da integra\u00e7\u00e3o com Pydantic.</p> <p>PostgreSQL (via Supabase): Adotado pela sua robustez, confiabilidade e capacidade de gerenciamento eficiente na nuvem.</p> <p>Render: Escolhido para implanta\u00e7\u00e3o da API e para o gerenciamento do Cron Job de automa\u00e7\u00e3o.</p> <p>Metabase (via Docker): Utilizado como ferramenta de Business Intelligence (BI) para visualiza\u00e7\u00e3o de dados, sendo uma solu\u00e7\u00e3o eficiente, flex\u00edvel e de c\u00f3digo aberto que pode ser executada localmente.</p> <p>Scikit-learn / Lifetimes: Bibliotecas amplamente reconhecidas no mercado. Enquanto a Scikit-learn foi usada para machine learning, a Lifetimes contribuiu para an\u00e1lise probabil\u00edstica de Lifetime Value, garantindo precis\u00e3o nas an\u00e1lises.</p>"},{"location":"#objetivos-estrategicos","title":"Objetivos Estrat\u00e9gicos","text":""},{"location":"#objetivos-tecnicos","title":"Objetivos T\u00e9cnicos","text":""},{"location":"#justificativa-das-tecnologias","title":"Justificativa das Tecnologias","text":""},{"location":"#modelo-preditivo-de-lead-scoring","title":"Modelo Preditivo de Lead Scoring","text":"<p>Esta fase focou em estruturar os dados brutos para que fossem compreens\u00edveis para um modelo de machine learning, com o objetivo de prever a probabilidade de convers\u00e3o de leads.</p> <p>O trabalho principal consistiu na cria\u00e7\u00e3o de uma tabela \u00fanica (feature_table), cuja cada linha representava um usu\u00e1rio e cada coluna, uma caracter\u00edstica descritiva. Destaque para:</p> <ol> <li> <p>O log de eventos foi consolidado por meio de pivot_table, transformando a\u00e7\u00f5es em contagens num\u00e9ricas (e.g., events_project_created).</p> </li> <li> <p>Informa\u00e7\u00f5es de marketing, como a campanha de origem, foram extra\u00eddas de campos JSON.</p> </li> <li> <p>A vari\u00e1vel-alvo (target_converted) foi criada a partir da coluna de status, convertendo-a em valores bin\u00e1rios (0 para \"n\u00e3o convertido\", 1 para \"convertido\").</p> </li> </ol> <p>A an\u00e1lise da feature_table forneceu importantes aprendizados:</p> <p>Dados Desequilibrados: Apenas 10,1% dos usu\u00e1rios eram convertidos, indicando forte desequil\u00edbrio que exigiria estrat\u00e9gias espec\u00edficas durante a modelagem.</p> <p>Impacto de Campanha: A campanha google_gestao_agil apresentou taxas de convers\u00e3o significativamente maiores, validando a hip\u00f3tese inicial.</p> <p>Multicolinearidade: Identificou-se forte correla\u00e7\u00e3o (0,96) e alto Fator de Infla\u00e7\u00e3o da Vari\u00e2ncia (VIF &gt; 25) entre as vari\u00e1veis events_trial_signup e events_user_verified, exigindo a remo\u00e7\u00e3o de uma delas para modelos sens\u00edveis.</p> <p>Foram treinados dois modelos e ambos integrados em um pipeline com as seguintes caracter\u00edsticas:</p> <p>Pr\u00e9-processamento: Incluiu a padroniza\u00e7\u00e3o de vari\u00e1veis num\u00e9ricas, codifica\u00e7\u00e3o de categorias e configura\u00e7\u00e3o de pesos balanceados (class_weight='balanced') devido ao desequil\u00edbrio nos dados.</p> <p>Regress\u00e3o Log\u00edstica (v1): Modelo simples, eficiente e interpret\u00e1vel.</p> <p>XGBoost (v2): Modelo avan\u00e7ado, com maior capacidade de aprendizado para dados complexos.</p> <p>Performance: Ambos os modelos apresentaram 100% em todas as m\u00e9tricas (AUC, precis\u00e3o e recall) devido \u00e0 clareza dos padr\u00f5es nos dados sint\u00e9ticos.</p> <p>Decis\u00e3o Final: Optou-se pela Regress\u00e3o Log\u00edstica, priorizando interpretabilidade, baixo custo computacional e robustez contra sobreajustes (overfitting).</p> <p>Esta \u00faltima etapa visou quantificar financeiramente o impacto das descobertas anteriores.</p> <p>Metodologia</p> <p>Foi criado um novo dataset sint\u00e9tico de transa\u00e7\u00f5es, no qual os clientes da campanha google_gestao_agil tinham maior frequ\u00eancia de compra e valores m\u00e9dios elevados. Dois modelos padr\u00e3o foram utilizados:</p> <p>BG/NBD: Para prever frequ\u00eancia e probabilidade de churn.</p> <p>Gamma-Gamma: Para prever o valor m\u00e9dio das transa\u00e7\u00f5es.</p>"},{"location":"#engenharia-de-atributos","title":"Engenharia de Atributos","text":""},{"location":"#analise-exploratoria-de-dados-eda","title":"An\u00e1lise Explorat\u00f3ria de Dados (EDA)","text":""},{"location":"#modelagem-e-avaliacao","title":"Modelagem e Avalia\u00e7\u00e3o","text":""},{"location":"#modelos-utilizados","title":"Modelos utilizados:","text":""},{"location":"#resultado-e-decisao-estrategica","title":"Resultado e Decis\u00e3o Estrat\u00e9gica","text":""},{"location":"#analise-de-impacto-financeiro-lifetime-value-ltv","title":"An\u00e1lise de Impacto Financeiro (Lifetime Value - LTV)","text":""},{"location":"#resultado-final","title":"Resultado Final","text":"<p>O LTV m\u00e9dio dos clientes da campanha google_gestao_agil foi 2,3 vezes maior ($73,60) do que o das campanhas regulares ($32). Essa an\u00e1lise forneceu \u03b7 base matem\u00e1tica para priorizar investimentos nesta campanha.</p>"},{"location":"01_arquitetura/","title":"Arquitetura","text":"<p>O sistema implementa um pipeline de dados completo que transforma cliques an\u00f4nimos em intelig\u00eancia de neg\u00f3cios acion\u00e1vel. O fluxo operacional consiste nas seguintes etapas: 1.  Coleta (Navegador): O arquivo tracker.js, configurado no site, captura eventos do usu\u00e1rio (por exemplo, page_view) e dados de atribui\u00e7\u00e3o (como UTMs). Ele gera um identificador \u00fanico (visitor_id) e armazena detalhes sobre intera\u00e7\u00f5es iniciais e finais (first touch/last touch) no localStorage do navegador. 2.  Ingest\u00e3o (API FastAPI): Os dados coletados s\u00e3o enviados por meio de requisi\u00e7\u00f5es POST para uma API constru\u00edda em FastAPI (main.py) e hospedada no Render. 3.  Valida\u00e7\u00e3o: A API utiliza os modelos do Pydantic para validar rigorosamente o payload dos eventos, garantindo ader\u00eancia ao esquema definido e rejeitando dados incompletos ou malformados. 4.  Armazenamento (PostgreSQL): Ap\u00f3s validados, os dados s\u00e3o inseridos de forma segura no banco de dados PostgreSQL, gerenciado via Supabase. 5.  An\u00e1lise e Modelagem (Python/Jupyter): Scripts em Python e notebooks Jupyter conectam-se ao banco de dados e realizam tarefas de engenharia de atributos (feature engineering) para treinar os modelos de aprendizado de m\u00e1quina (Lead Scoring e LTV). 6.  Automa\u00e7\u00e3o (Cron Job): Um script chamado score_updater.py \u00e9 executado diariamente no Render. Ele carrega o modelo treinado, verifica novos usu\u00e1rios, calcula suas pontua\u00e7\u00f5es e atualiza os elementos necess\u00e1rios no banco de dados. 7.  Visualiza\u00e7\u00e3o (Metabase): O Metabase, rodando localmente por meio de cont\u00eainer Docker, conecta-se ao banco de dados (em modo de leitura somente) para gerar dashboards e relat\u00f3rios interativos.</p> <p>O diagrama abaixo ilustra o fluxo completo de dados dentro do sistema InsightOS, desde a coleta inicial at\u00e9 a visualiza\u00e7\u00e3o final. Figura 1 - Diagrama de Fluxo de Dados do Sistema InsightOS </p>"},{"location":"01_arquitetura/#20-arquitetura-do-sistema","title":"2.0 Arquitetura do sistema","text":""},{"location":"01_arquitetura/#21-pipeline-de-dados","title":"2.1 Pipeline de Dados","text":""},{"location":"01_arquitetura/#22-tecnologias-utilizadas","title":"2.2 Tecnologias Utilizadas","text":"<p>A tabela a seguir apresenta as tecnologias-chave do sistema e suas respectivas justificativas: | Categoria         | Tecnologia          | Justificativa                                                                 | |-----------------------|-------------------------|----------------------------------------------------------------------------------| | Coleta           | JavaScript (custom)    | Script tracker.js criado para captura de eventos no front-end e atribui\u00e7\u00e3o de origem. | | Ingest\u00e3o (API)   | Python / FastAPI       | API simult\u00e2nea usada para valida\u00e7\u00e3o dos dados (com Pydantic) e inser\u00e7\u00e3o segura no banco. | | Armazenamento    | PostgreSQL (Supabase)  | Banco de dados relacional gerenciado na nuvem, servindo como reposit\u00f3rio central dos dados. | | An\u00e1lise/Modelagem| Python / Jupyter       | Ferramentas utilizadas para realizar engenharia de atributos e treinar os modelos. | | Automa\u00e7\u00e3o        | Cron Job (Render)      | Executa diariamente o script score_updater.py para atualizar pontua\u00e7\u00f5es no banco. | | Visualiza\u00e7\u00e3o     | Metabase (via Docker)  | Ferramenta de BI open-source, utilizada para criar dashboards e an\u00e1lises visuais. |</p>"},{"location":"01_arquitetura/#23-diagrama-do-sistema","title":"2.3 Diagrama do Sistema","text":""},{"location":"02_dados_sinteticos/","title":"Gera\u00e7\u00e3o de Dados","text":""},{"location":"02_dados_sinteticos/#3-geracao-de-dados-sinteticos","title":"3. Gera\u00e7\u00e3o de Dados Sint\u00e9ticos","text":""},{"location":"02_dados_sinteticos/#31-justificativa","title":"3.1 Justificativa","text":"<p>A gera\u00e7\u00e3o de dados sint\u00e9ticos foi uma decis\u00e3o estrat\u00e9gica adotada na fase inicial do projeto. A principal raz\u00e3o foi a necessidade de criar um ambiente de laborat\u00f3rio controlado, que permitisse realizar testes rigorosos sem as limita\u00e7\u00f5es associadas aos dados do mundo real, como escassez ou excesso de ru\u00eddos. Essa abordagem possibilitou: 1. Validar o Pipeline: Garantir que o pipeline de ponta a ponta (coleta, API, banco de dados e visualiza\u00e7\u00e3o em BI) estava funcionando corretamente. 2. Criar Padr\u00f5es Realistas: Injetar intencionalmente padr\u00f5es e correla\u00e7\u00f5es plaus\u00edveis nos dados. Por exemplo, simular que a campanha google_gestao_agil apresenta uma maior taxa de convers\u00e3o. 3. Testar Modelos: Construir um \"quebra-cabe\u00e7as\" com uma solu\u00e7\u00e3o previamente conhecida, para verificar se os modelos de machine learning (ML) e Lifetime Value (LTV) eram capazes de identificar os padr\u00f5es incorporados.</p>"},{"location":"02_dados_sinteticos/#32-implementacao","title":"3.2 Implementa\u00e7\u00e3o","text":"<p>O processo de gera\u00e7\u00e3o de dados foi elaborado no notebook 01_data_simulation.ipynb, utilizando as bibliotecas pandas para manipula\u00e7\u00e3o de DataFrames e faker para criar dados fict\u00edcios, mas com apar\u00eancia real, como nomes e endere\u00e7os de e-mail.</p>"},{"location":"02_dados_sinteticos/#1-simulacao-do-funil-de-conversao-lead-scoring","title":"1. Simula\u00e7\u00e3o do Funil de Convers\u00e3o (Lead Scoring)","text":"<p>A estrat\u00e9gia central foi a cria\u00e7\u00e3o de um funil probabil\u00edstico, no qual eventos-chave dependiam de uma cascata de decis\u00f5es baseadas em probabilidades condicionais. Essa abordagem permitiu modelar o comportamento de usu\u00e1rios de forma realista, em vez de utilizar eventos aleat\u00f3rios.</p> <p>python</p>"},{"location":"02_dados_sinteticos/#logica-de-transicao","title":"L\u00f3gica de Transi\u00e7\u00e3o","text":"<p>A l\u00f3gica foi projetada para que a progress\u00e3o de um usu\u00e1rio no funil fosse dependente de a\u00e7\u00f5es anteriores e de fatores contextuais, como a campanha de origem. Por exemplo: * Um usu\u00e1rio s\u00f3 poderia criar um projeto ap\u00f3s verificar seu e-mail.  A probabilidade de realizar a a\u00e7\u00e3o de criar um projeto era configurada como condicional \u00e0 campanha de aquisi\u00e7\u00e3o do usu\u00e1rio.  Isso permitiu representar o comportamento simulado de campanhas que atraem usu\u00e1rios mais qualificados. O trecho de c\u00f3digo abaixo ilustra essa l\u00f3gica:</p>"},{"location":"02_dados_sinteticos/#logica-de-simulacao-do-funil","title":"L\u00f3gica de simula\u00e7\u00e3o do funil","text":""},{"location":"02_dados_sinteticos/#probabilidade-de-criacao-de-projeto-depende-da-campanha-de-aquisicao","title":"Probabilidade de cria\u00e7\u00e3o de projeto depende da campanha de aquisi\u00e7\u00e3o","text":"<p>prob_create_project = 0.55 if user['acquisition_campaign'] == 'google_gestao_agil' else 0.30</p>"},{"location":"02_dados_sinteticos/#o-evento-so-ocorre-se-a-condicao-probabilistica-for-satisfeita","title":"O evento s\u00f3 ocorre se a condi\u00e7\u00e3o probabil\u00edstica for satisfeita","text":"<p>if random.random() &lt; prob_create_project:     # Gera o evento 'project_created' no funil     events.append(...) </p> <pre><code># L\u00f3gica aninhada para a pr\u00f3xima etapa do funil (assinatura)\nprob_subscribe = 0.40 if user['acquisition_campaign'] == 'google_gestao_agil' else 0.15\nif random.random() &lt; prob_subscribe:\n    events.append(...)\n    # Atualiza o status do usu\u00e1rio na base de dados\n    users_df.loc[users_df['user_id'] == user_id, 'status'] = 'converted'\n</code></pre> <ol> <li>Duplicate key value... users_pkey:    Esse erro ocorreu devido \u00e0 tentativa de inserir registros duplicados na chave prim\u00e1ria da tabela users. A solu\u00e7\u00e3o foi limpar as tabelas antes de cada rodada de inser\u00e7\u00e3o para garantir que o ambiente de teste estivesse limpo e sem dados residuais.</li> <li>Duplicate key value... users_email_key:    Esse problema ocorreu devido \u00e0 gera\u00e7\u00e3o de endere\u00e7os de e-mail duplicados pela biblioteca faker. A solu\u00e7\u00e3o foi implementar uma l\u00f3gica de verifica\u00e7\u00e3o, garantindo que nenhum e-mail repetido fosse gerado durante o processo de simula\u00e7\u00e3o.</li> <li>Out of range float values...:    Esse problema foi causado pela incompatibilidade entre valores np.nan (gerados pelo Pandas) e o formato JSON requerido pelo banco de dados, que espera null. A solu\u00e7\u00e3o envolveu converter os valores np.nan em None (nativo do Python), que \u00e9 compat\u00edvel com o formato null do PostgreSQL.</li> </ol> <p>A valida\u00e7\u00e3o final dos dados foi realizada de forma visual por meio do Metabase. Um gr\u00e1fico de funil (Figura 2) foi gerado para verificar se toda a l\u00f3gica probabil\u00edstica foi corretamente aplicada e armazenada na base de dados. A an\u00e1lise confirmou que os padr\u00f5es simulados estavam sendo representados\u00a0adequadamente.</p> <p></p>"},{"location":"02_dados_sinteticos/#2-simulacao-de-transacoes-ltv","title":"2. Simula\u00e7\u00e3o de Transa\u00e7\u00f5es (LTV)","text":"<p>Para a an\u00e1lise de Lifetime Value (LTV), foi desenvolvido um segundo script que gerava um log de transa\u00e7\u00f5es simuladas. Assim como na simula\u00e7\u00e3o do funil de convers\u00e3o, os dados foram enviesados intencionalmente para refletir diferen\u00e7as entre os clientes de diferentes campanhas. Destaques da configura\u00e7\u00e3o: * Maior probabilidade de recompra: Clientes provenientes da campanha google_gestao_agil apresentaram maior chance de realizar compras recorrentes. Ciclo de compra mais curto: Esses clientes realizaram compras em intervalos menores. Ticket m\u00e9dio superior: Os valores monet\u00e1rios por transa\u00e7\u00e3o foram configurados para serem maiores em compara\u00e7\u00e3o \u00e0s demais campanhas.</p>"},{"location":"02_dados_sinteticos/#33-avaliacao-da-qualidade-e-desafios","title":"3.3 Avalia\u00e7\u00e3o da Qualidade e Desafios","text":"<p>A qualidade dos dados foi validada com base na sua capacidade de serem inseridos corretamente no banco de dados e analisados posteriormente. Durante o processo de inser\u00e7\u00e3o, foram enfrentados e resolvidos alguns desafios t\u00e9cnicos:</p>"},{"location":"03_lead_scoring/","title":"Modelo: Lead Scoring","text":""},{"location":"03_lead_scoring/#4-modelagem-de-lead-scoring","title":"4. Modelagem de Lead Scoring","text":""},{"location":"03_lead_scoring/#41-estrategia","title":"4.1 Estrat\u00e9gia","text":"<p>O objetivo deste m\u00f3dulo foi prever a vari\u00e1vel-alvo target_converted (1 para usu\u00e1rios convertidos e 0 para n\u00e3o convertidos). A estrat\u00e9gia de modelagem foi dividida nos seguintes passos: 1. Engenharia de Atributos: Transforma\u00e7\u00e3o dos logs de eventos brutos (user_events) em uma feature_table est\u00e1tica, onde cada linha corresponde a um usu\u00e1rio e cada coluna representa uma caracter\u00edstica calculada, como a contagem de eventos do tipo project_created. 2. Tratamento de Desequil\u00edbrio: A an\u00e1lise explorat\u00f3ria de dados (EDA) revelou um desequil\u00edbrio significativo na vari\u00e1vel-alvo: apenas 10,1% dos usu\u00e1rios eram classificados como \"convertidos\". Para lidar com esse desequil\u00edbrio, foi utilizada a configura\u00e7\u00e3o class_weight='balanced' no modelo, garantindo maior relev\u00e2ncia \u00e0s observa\u00e7\u00f5es da classe minorit\u00e1ria e prevenindo que o modelo ignorasse essa classe. 3. Experimenta\u00e7\u00e3o Comparativa: Dois modelos foram comparados: um baseline simples e interpret\u00e1vel (Regress\u00e3o Log\u00edstica) e um modelo mais complexo e robusto (XGBoost). Essa abordagem permitiu avaliar tanto performance quanto facilidade de interpreta\u00e7\u00e3o. 4. Decis\u00e3o Baseada na Parcim\u00f4nia: Como ambos os modelos atingiram 100% de acur\u00e1cia nos dados sint\u00e9ticos, foi aplicado o Princ\u00edpio da Parcim\u00f4nia. Assim, optou-se pelo modelo mais simples (Regress\u00e3o Log\u00edstica) devido \u00e0s suas caracter\u00edsticas de maior interpretabilidade, efici\u00eancia computacional e menor propens\u00e3o ao overfitting.</p>"},{"location":"03_lead_scoring/#42-detalhes-tecnicos","title":"4.2 Detalhes T\u00e9cnicos","text":"<p>O processo de modelagem foi encapsulado em um Pipeline do Scikit-learn, assegurando consist\u00eancia na aplica\u00e7\u00e3o do pr\u00e9-processamento durante todas as etapas de treinamento e infer\u00eancia.</p> <p>python from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.linear_model import LogisticRegression</p>"},{"location":"03_lead_scoring/#1-pipeline-de-pre-processamento","title":"1. Pipeline de Pr\u00e9-processamento","text":"<p>Foi utilizado um ColumnTransformer para aplicar transforma\u00e7\u00f5es distintas \u00e0s diferentes categorias de vari\u00e1veis: * Colunas Num\u00e9ricas: Para vari\u00e1veis como events_project_created, foi aplicada a t\u00e9cnica de padroniza\u00e7\u00e3o com StandardScaler(), ajustando os dados para que fiquem em uma escala comum. Colunas Categ\u00f3ricas:* Os atributos categ\u00f3ricos, como campaign, foram convertidos em vari\u00e1veis num\u00e9ricas (codifica\u00e7\u00e3o 0/1) utilizando OneHotEncoder(). O par\u00e2metro handle_unknown='ignore' foi configurado para tornar o modelo robusto a novas categorias (ex.: campanhas in\u00e9ditas) que possam surgir no futuro.</p>"},{"location":"03_lead_scoring/#2-estrutura-do-pipeline","title":"2. Estrutura do Pipeline","text":"<p>Abaixo est\u00e1 o c\u00f3digo Python utilizado para estruturar o pipeline final, integrando o pr\u00e9-processamento com o modelo de classifica\u00e7\u00e3o.</p>"},{"location":"03_lead_scoring/#definir-as-colunas-para-cada-tipo-de-transformacao","title":"Definir as colunas para cada tipo de transforma\u00e7\u00e3o","text":"<p>numeric_features = ['events_project_created', 'events_subscription_started', ...] categorical_features = ['campaign', 'plan', ...]</p>"},{"location":"03_lead_scoring/#criar-o-pre-processador","title":"Criar o pr\u00e9-processador","text":"<p>preprocessor = ColumnTransformer(     transformers=[         ('num', StandardScaler(), numeric_features),         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)     ])</p>"},{"location":"03_lead_scoring/#definir-o-pipeline-final-regressao-logistica","title":"Definir o pipeline final (Regress\u00e3o Log\u00edstica)","text":"<p>lr_pipeline = Pipeline(steps=[     ('preprocessor', preprocessor),     ('classifier', LogisticRegression(class_weight='balanced', random_state=42)) ])</p>"},{"location":"03_lead_scoring/#treinar-o-modelo-com-os-dados-de-treino","title":"Treinar o modelo com os dados de treino","text":"<p>lr_pipeline.fit(X_train, y_train)</p>"},{"location":"03_lead_scoring/#salvar-o-pipeline-inteiro-para-producao","title":"Salvar o pipeline inteiro para produ\u00e7\u00e3o","text":"<p>import joblib joblib.dump(lr_pipeline, 'lead_scoring_pipeline_v1.joblib')</p>"},{"location":"03_lead_scoring/#43-analise-visual-de-resultados-e-insights","title":"4.3 An\u00e1lise Visual de Resultados e Insights","text":""},{"location":"03_lead_scoring/#3-metricas-de-avaliacao","title":"3. M\u00e9tricas de Avalia\u00e7\u00e3o","text":"<p>A avalia\u00e7\u00e3o robusta, realizada por meio de teste \u00fanico e valida\u00e7\u00e3o cruzada, confirmou uma performance perfeita do pipeline nos dados simulados. Os principais resultados observados foram: * AUC (\u00c1rea Sob a Curva ROC): 1.0000 Precis\u00e3o: 1.0000 Recall (Sensibilidade): 1.0000 Desvio Padr\u00e3o (Valida\u00e7\u00e3o Cruzada):* 0.0000 (indicando estabilidade total do modelo durante a valida\u00e7\u00e3o).  </p> <p>Essa performance perfeita validou que o pipeline t\u00e9cnico era 100% funcional e capaz de solucionar o problema \"f\u00e1cil\" representado pelos dados sint\u00e9ticos. Al\u00e9m disso, o log do Modelo v2 (baseado em XGBoost) confirmou resultados id\u00eanticos, conforme ilustrado na imagem abaixo, extra\u00edda do arquivo modelo_v2.png:</p> <p></p> <p>A tabela de compara\u00e7\u00e3o final (extra\u00edda do arquivo Celula_11.png) resume os resultados da \"batalha\" entre os Modelos v1 (Regress\u00e3o Log\u00edstica) e v2 (XGBoost). Ambos modelos alcan\u00e7aram performance id\u00eantica, o que refor\u00e7ou a aplica\u00e7\u00e3o do Princ\u00edpio da Parcim\u00f4nia e a escolha do Modelo v1 pela sua simplicidade e interpretabilidade: </p>"},{"location":"03_lead_scoring/#1-funil-de-conversao-contexto-da-eda","title":"1. Funil de Convers\u00e3o (Contexto da EDA)","text":"<p>Durante a etapa de An\u00e1lise Explorat\u00f3ria de Dados (EDA), foi gerado um gr\u00e1fico de funil que contextualizou o problema e identificou o principal ponto de atrito na jornada do cliente. O funil destacou onde ocorre a maior quebra de engajamento entre etapas cr\u00edticas no fluxo do usu\u00e1rio: * Ponto Cr\u00edtico: A maior quebra foi observada entre as etapas user_verified e project_created. Apenas 41,5% dos usu\u00e1rios engajados avan\u00e7aram nesse ponto da jornada, representando uma redu\u00e7\u00e3o significativa. Implica\u00e7\u00e3o:* Essa etapa (project_created) foi identificada como um dos preditores mais relevantes para o modelo de lead scoring. O gr\u00e1fico abaixo, retirado do arquivo funil_conversao.png, ilustra o funil completo de convers\u00e3o: </p>"},{"location":"03_lead_scoring/#2-distribuicao-dos-scores-do-modelo","title":"2. Distribui\u00e7\u00e3o dos Scores do Modelo","text":"<p>Um gr\u00e1fico foi gerado para visualizar a distribui\u00e7\u00e3o dos scores preditivos do Modelo v1 (Regress\u00e3o Log\u00edstica) aplicados sobre os 1.000 usu\u00e1rios presentes nos dados. Os principais resultados encontrados foram: * Descoberta: A distribui\u00e7\u00e3o final dos scores foi marcadamente bimodal, indicando que o modelo tem alta capacidade de decis\u00e3o e segmenta\u00e7\u00e3o. Grupo Frio (Score \u2248 0.0): Aproximadamente 90% dos leads foram classificados com alta confian\u00e7a como \"frias\". Grupo Quente (Score \u2248 1.0): Cerca de 10% dos leads foram classificados com alta confian\u00e7a como \"quentes\". Implica\u00e7\u00e3o:* A aus\u00eancia de scores intermedi\u00e1rios (como 0.4 ou 0.5) elimina ambiguidades e permite que a equipe comercial concentre seus esfor\u00e7os exclusivamente no grupo \"quente\", otimizando a efici\u00eancia operacional. O gr\u00e1fico abaixo, extra\u00eddo do arquivo distribuicao_scores.png, ilustra essa distribui\u00e7\u00e3o dos scores de lead scoring: </p>"},{"location":"04_analise_ltv/","title":"Modelo: An\u00e1lise de LTV","text":""},{"location":"04_analise_ltv/#5-analise-financeira-ltv","title":"5. An\u00e1lise Financeira (LTV)","text":""},{"location":"04_analise_ltv/#51-estrategia","title":"5.1 Estrat\u00e9gia","text":"<p>O objetivo desta etapa foi quantificar financeiramente o valor dos leads e demonstrar o impacto das campanhas de marketing nos resultados financeiros. Para isso, utilizamos modelos probabil\u00edsticos padr\u00e3o amplamente adotados na ind\u00fastria: 1. Modelo BG/NBD (Beta-Geometric/Negative-Binomial-Distribution): Este modelo foi utilizado para prever a atividade futura do cliente, respondendo perguntas como: \"Quantas compras futuras podemos esperar?\" e \"Qual a probabilidade de o cliente estar 'ativo' (n\u00e3o ter sofrido churn)?\". 2. Modelo Gamma-Gamma: Aplicado para estimar o valor monet\u00e1rio m\u00e9dio das transa\u00e7\u00f5es futuras, considerando apenas os clientes que est\u00e3o \"ativos\" (identificados pelo modelo BG/NBD). 3. C\u00e1lculo do LTV (Lifetime Value): O LTV foi calculado combinando as sa\u00eddas dos dois modelos, atrav\u00e9s da f\u00f3rmula: LTV = Frequ\u00eancia Esperada x Valor Monet\u00e1rio Esperado.</p>"},{"location":"04_analise_ltv/#52-descoberta-critica-violacao-de-premissa","title":"5.2 Descoberta Cr\u00edtica (Viola\u00e7\u00e3o de Premissa)","text":"<p>Antes de treinar o modelo Gamma-Gamma, \u00e9 essencial verificar se suas premissas foram atendidas. Esse modelo assume que n\u00e3o h\u00e1 correla\u00e7\u00e3o entre a frequ\u00eancia de compras de um cliente e o valor m\u00e9dio de suas transa\u00e7\u00f5es. Contudo, nossa an\u00e1lise apontou uma viola\u00e7\u00e3o dessa premissa: * Nossa An\u00e1lise: Detectamos uma correla\u00e7\u00e3o de 0.3882 (aproximadamente 0.39), o que ser\u00e1 evidenciado na imagem de log abaixo. Tradu\u00e7\u00e3o para o Neg\u00f3cio: Esse resultado implica que \"clientes mais leais s\u00e3o tamb\u00e9m os que gastam mais\". Embora isso seja um insight valioso para decis\u00f5es estrat\u00e9gicas, ele viola a premissa fundamental do modelo. * Decis\u00e3o Estrat\u00e9gica: Apesar da viola\u00e7\u00e3o, optamos por prosseguir com o modelo. Em vez de tratar o LTV como uma previs\u00e3o cont\u00e1bil exata, interpretamos seus resultados como um \u00edndice comparativo robusto*. Dessa forma, o LTV ser\u00e1 usado principalmente para ranquear o valor relativo das campanhas e orientar investimentos estrat\u00e9gicos.</p>"},{"location":"04_analise_ltv/#53-sumarios-de-performance-dos-modelos","title":"5.3 Sum\u00e1rios de Performance dos Modelos","text":""},{"location":"04_analise_ltv/#1-sumario-do-modelo-bgnbd-frequencia-e-churn","title":"1. Sum\u00e1rio do Modelo BG/NBD (Frequ\u00eancia e Churn)","text":"<p>O primeiro modelo diagnosticou o comportamento \"m\u00e9dio\" da base de clientes, fornecendo informa\u00e7\u00f5es como a frequ\u00eancia esperada de compras e a taxa de churn. A imagem abaixo mostra os resultados gerados pelo modelo:</p>"},{"location":"04_analise_ltv/#_1","title":"Modelo: An\u00e1lise de LTV","text":""},{"location":"04_analise_ltv/#2-sumario-do-modelo-gamma-gamma-valor-monetario","title":"2. Sum\u00e1rio do Modelo Gamma-Gamma (Valor Monet\u00e1rio)","text":"<p>Este modelo foi utilizado para estimar o valor monet\u00e1rio m\u00e9dio das transa\u00e7\u00f5es futuras. A imagem abaixo comprova a nossa \"Descoberta Cr\u00edtica\" sobre a correla\u00e7\u00e3o entre frequ\u00eancia de compra e valor monet\u00e1rio m\u00e9dio, mostrando os seguintes resultados: * Primeira Linha: Confirma a Correla\u00e7\u00e3o de 0.3882, evidenciando a viola\u00e7\u00e3o da premissa inicial do modelo. Linhas Seguintes:* Exibem o sum\u00e1rio do modelo de valor monet\u00e1rio, detalhando os par\u00e2metros t\u00e9cnicos estimados.</p>"},{"location":"04_analise_ltv/#_2","title":"Modelo: An\u00e1lise de LTV","text":""},{"location":"04_analise_ltv/#54-visualizacao-preditiva-heatmap","title":"5.4 Visualiza\u00e7\u00e3o Preditiva (Heatmap)","text":"<p>O modelo BG/NBD tamb\u00e9m permite uma visualiza\u00e7\u00e3o detalhada da \"sa\u00fade\" esperada da base de clientes. O heatmap (mapa de calor) abaixo apresenta o n\u00famero esperado de compras futuras no pr\u00f3ximo per\u00edodo, fornecendo insights sobre o comportamento de diferentes segmentos de clientes: * Eixo X (Frequ\u00eancia Hist\u00f3rica): Representa os clientes que compraram mais vezes no passado. Eixo Y (Rec\u00eancia): Representa os clientes que realizaram compras mais recentemente (aqueles posicionados mais abaixo no gr\u00e1fico). Insight (Mancha Clara): O segmento no canto inferior direito \u2014 formado por clientes recentes e frequentes \u2014 apresenta a maior probabilidade de realizar novas compras. O modelo confirma que esses s\u00e3o os melhores clientes a serem priorizados.</p>"},{"location":"04_analise_ltv/#_3","title":"Modelo: An\u00e1lise de LTV","text":""},{"location":"04_analise_ltv/#55-resultados-preditivos-top-10-clientes","title":"5.5 Resultados Preditivos (Top 10 Clientes)","text":"<p>Combinando os resultados do BG/NBD e do Gamma-Gamma, foi poss\u00edvel calcular o LTV futuro estimado para cada cliente. A tabela abaixo mostra os 10 melhores clientes classificados pelo LTV:</p> <p></p> <ul> <li>Descoberta: Todos os 10 clientes com maior LTV estimado pertencem \u00e0 campanha google_gestao_agil.  </li> <li>Valida\u00e7\u00e3o: Isso confirma, de forma inequ\u00edvoca, que a metodologia adotada foi capaz de identificar corretamente os clientes de maior valor. Os padr\u00f5es intencionalmente inseridos nos dados sint\u00e9ticos foram reconhecidos pelos modelos, validando a solu\u00e7\u00e3o proposta.</li> </ul>"},{"location":"04_analise_ltv/#56-conclusao-financeira-agregacao-por-campanha","title":"5.6 Conclus\u00e3o Financeira (Agrega\u00e7\u00e3o por Campanha)","text":"<p>Por fim, agrupamos o LTV m\u00e9dio por campanha para quantificar os resultados financeiros e validar as recomenda\u00e7\u00f5es sugeridas pelo modelo de lead scoring. O resultado consolidado est\u00e1 descrito abaixo: * Prova Financeira: O LTV m\u00e9dio de um cliente da campanha google_gestao_agil \u00e9 de R$73.60. ROI Comparativo: Esse valor \u00e9 2.3 vezes maior (ou 130% superior) ao LTV m\u00e9dio estimado para as demais campanhas. Recomenda\u00e7\u00e3o Estrat\u00e9gica: A an\u00e1lise financeira fornece uma prova matem\u00e1tica inequ\u00edvoca para justificar a realoca\u00e7\u00e3o do or\u00e7amento de marketing. Nesse caso, recomendamos redirecionar investimentos para a campanha google_gestao_agil, que gera os clientes de maior valor.</p>"},{"location":"04_analise_ltv/#_4","title":"Modelo: An\u00e1lise de LTV","text":"<ol> <li>Consist\u00eancia dos Modelos: Apesar da viola\u00e7\u00e3o de premissas, a metodologia proposta (BG/NBD + Gamma-Gamma) foi capaz de ranquear as campanhas e os clientes de maneira robusta, oferecendo insights valiosos para o neg\u00f3cio.</li> <li>Recomenda\u00e7\u00f5es Estrat\u00e9gicas: A campanha google_gestao_agil se destaca como a mais eficiente na gera\u00e7\u00e3o de leads de alto valor. A realoca\u00e7\u00e3o do or\u00e7amento de outras campanhas para essa estrat\u00e9gia deve maximizar o ROI do projeto.</li> <li>Pr\u00f3ximos Passos: Para futuras an\u00e1lises reais, \u00e9 recomend\u00e1vel explorar modelos complementares ou ajustados que n\u00e3o dependam da suposi\u00e7\u00e3o de independ\u00eancia entre frequ\u00eancia de compra e valor monet\u00e1rio. Isso pode melhorar a precis\u00e3o absoluta\u00a0do\u00a0LTV.</li> </ol>"},{"location":"04_analise_ltv/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":""},{"location":"05_automacao/","title":"Automa\u00e7\u00e3o (Produ\u00e7\u00e3o)","text":""},{"location":"05_automacao/#6-automacao-producao","title":"6. Automa\u00e7\u00e3o (Produ\u00e7\u00e3o)","text":""},{"location":"05_automacao/#61-scripts-automatizados","title":"6.1 Scripts Automatizados","text":"<p>A etapa final do projeto foi transformar o modelo de Lead Scoring em um sistema automatizado e operacional, capaz de integrar-se ao fluxo de trabalho existente e funcionar de forma aut\u00f3noma como parte do neg\u00f3cio.</p>"},{"location":"05_automacao/#configuracao-geral","title":"Configura\u00e7\u00e3o Geral","text":"<ul> <li>Script Implementado: score_updater.py.  </li> <li>Plataforma de Automa\u00e7\u00e3o: Cron Job utilizando Render.  </li> <li>Tarefa Definida: </li> <li>Executar diariamente.  </li> <li>Conectar-se ao banco de dados (BD).  </li> <li>Identificar os utilizadores sem pontua\u00e7\u00e3o de lead (lead_score).  </li> <li>Carregar o pipeline previamente treinado (lead_scoring_pipeline_v1.joblib) para realizar previs\u00f5es.  </li> <li>Atualizar a coluna lead_score na tabela users.</li> </ul>"},{"location":"05_automacao/#62-desafio-de-producao-e-solucao-processamento-em-lotes","title":"6.2 Desafio de Produ\u00e7\u00e3o e Solu\u00e7\u00e3o: Processamento em Lotes","text":"<p>Durante os testes locais do script de produ\u00e7\u00e3o, foi identificado um problema operacional: Erro Encontrado: 414 Request-URI Too Large. Causa: O script tentava buscar os eventos de todos os utilizadores (1.000) em uma \u00fanica requisi\u00e7\u00e3o \u00e0 API, o que gerava uma URL muito longa para o servidor lidar.</p>"},{"location":"05_automacao/#solucao-implementada","title":"Solu\u00e7\u00e3o Implementada:","text":"<p>Foi adicionada uma l\u00f3gica de processamento em lotes (batch processing) para dividir os utilizadores em grupos menores e process\u00e1-los separadamente. Essa abordagem permitiu ao sistema operar de maneira mais eficiente e evitar sobrecarga.  Os lotes foram definidos com um tamanho fixo de 100 utilizadores, e o script foi ajustado para processar cada lote de forma independente. Esse ajuste garantiu estabilidade e robustez na opera\u00e7\u00e3o automatizada.</p>"},{"location":"05_automacao/#execucao-local-validacao","title":"Execu\u00e7\u00e3o Local: Valida\u00e7\u00e3o","text":"<p>O script foi testado localmente em 10 lotes de 100 utilizadores, para validar a l\u00f3gica implementada. As imagens abaixo ilustram diferentes momentos do processo: In\u00edcio do Processo (Lote 1 de 10): Meio do Processo (Lote 5 de 10): Fim do Processo (Lote 10 de 10): </p>"},{"location":"05_automacao/#63-monitoramento-e-desafios-de-implantacao","title":"6.3 Monitoramento e Desafios de Implanta\u00e7\u00e3o","text":"<p>Ap\u00f3s validar o script localmente, foi realizado o deploy na plataforma Render. Esse processo foi monitorado via o dashboard de \"Events\", que revelou os desafios cl\u00e1ssicos encontrados no mundo real durante a implanta\u00e7\u00e3o de sistemas em produ\u00e7\u00e3o.</p>"},{"location":"05_automacao/#desafios-identificados-e-solucoes","title":"Desafios Identificados e Solu\u00e7\u00f5es:","text":"<ol> <li>Erro (07:37:35 PM): ModuleNotFoundError: No module named 'pandas' </li> <li>Causa: O arquivo requirements.txt enviado para o Render n\u00e3o inclu\u00eda depend\u00eancias fundamentais como pandas e numpy.  </li> <li>Solu\u00e7\u00e3o: As bibliotecas foram adicionadas ao arquivo requirements.txt e um novo commit foi enviado ao reposit\u00f3rio.  </li> <li>Erro (07:47:28 PM): Invalid API key </li> <li>Causa: Ap\u00f3s corrigir o primeiro bug, o script falhou ao conectar-se ao Supabase devido a credenciais incorretas (SUPABASE_KEY) configuradas nas Vari\u00e1veis de Ambiente do Render.  </li> <li>Solu\u00e7\u00e3o: As credenciais foram corrigidas diretamente no dashboard de vari\u00e1veis de ambiente do Render.  </li> </ol>"},{"location":"05_automacao/#validacao-do-build-e-execucao-bem-sucedida","title":"Valida\u00e7\u00e3o do Build e Execu\u00e7\u00e3o Bem-Sucedida","text":"<p>Ap\u00f3s implementar as corre\u00e7\u00f5es descritas acima, o build foi executado com sucesso. O dashboard do Render confirmou que todas as depend\u00eancias estavam instaladas corretamente e que o script estava pronto para execu\u00e7\u00e3o.  A execu\u00e7\u00e3o inicial do Cron Job ocorreu \u00e0s 08:18:56 PM sem erros (\"Process completed\"). O resultado final foi validado diretamente no banco de dados Supabase, onde a coluna lead_score foi preenchida corretamente para todos os utilizadores. </p>"},{"location":"05_automacao/#64-prova-de-execucao-autonoma-confiabilidade-do-sistema","title":"6.4 Prova de Execu\u00e7\u00e3o Aut\u00f3noma: Confiabilidade do Sistema","text":"<p>Para comprovar que o sistema automatizado \u00e9 confi\u00e1vel a longo prazo, os logs de eventos do Render foram consultados ap\u00f3s v\u00e1rios dias da implementa\u00e7\u00e3o. O cron job foi executado diariamente e pontualmente, conforme esperado, nos dias 14, 15 e 16 de setembro, sem interrup\u00e7\u00f5es. </p>"},{"location":"05_automacao/#resultados-consolidados","title":"Resultados Consolidados:","text":"<ul> <li>O pipeline foi capaz de gerar scores para novos utilizadores automaticamente, sem interven\u00e7\u00e3o manual.  </li> <li>O sistema demonstrou alta confiabilidade, processando as previs\u00f5es sem falhas consecutivas desde o primeiro deploy, validando a robustez do processo automatizado.</li> </ul>"},{"location":"05_automacao/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":"<ol> <li>L\u00f3gica de Batch Processing: A solu\u00e7\u00e3o desenvolvida para dividir o processamento em lotes de 100 garantiu estabilidade e efici\u00eancia, resolvendo o problema encontrado nos testes locais.  </li> <li>Monitoramento e Corre\u00e7\u00e3o em Produ\u00e7\u00e3o: Os desafios de produ\u00e7\u00e3o foram diagnosticados e corrigidos rapidamente, demonstrando a robustez e a adaptabilidade da equipe frente a problemas comuns do mundo real.  </li> <li>Automa\u00e7\u00e3o e Escalabilidade: O cron job configurado no Render est\u00e1 funcionando diariamente de forma aut\u00f3noma e pode escalar facilmente para bases de dados maiores, bastando ajustar o tamanho dos lotes ou os hor\u00e1rios de execu\u00e7\u00e3o.  </li> <li>Pr\u00f3ximos Passos: </li> <li>Continuar monitorando a execu\u00e7\u00e3o do cron job atrav\u00e9s do dashboard de eventos para antecipar poss\u00edveis falhas futuras.  </li> <li>Incluir m\u00e9tricas de performance na monitoriza\u00e7\u00e3o dos lotes processados para garantir otimiza\u00e7\u00e3o\u00a0cont\u00ednua.</li> </ol>"}]}