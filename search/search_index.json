{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vis\u00e3o Geral do Projeto InsightOS","text":""},{"location":"#11-introducao","title":"1.1 Introdu\u00e7\u00e3o","text":""},{"location":"#resumo-executivo","title":"Resumo Executivo","text":"<p>Este documento detalha o ciclo de vida completo do projeto InsightOS, uma iniciativa para transformar dados brutos de utilizadores em intelig\u00eancia de neg\u00f3cio acion\u00e1vel. O desafio central era a inefici\u00eancia na aquisi\u00e7\u00e3o de clientes da plataforma SaaS InsightOS, que n\u00e3o conseguia distinguir leads de alto e baixo potencial.</p> <p>A solu\u00e7\u00e3o foi a constru\u00e7\u00e3o de um sistema de dados de ponta a ponta, centrado num modelo de machine learning de Lead Scoring e complementado por uma an\u00e1lise de Lifetime Value (LTV). O sistema rastreia a jornada do utilizador, calcula uma pontua\u00e7\u00e3o de probabilidade de convers\u00e3o e, por fim, automatiza esse processo.</p> <p>O resultado \u00e9 um sistema robusto que permite \u00e0 InsightOS identificar os leads de maior potencial, otimizar os investimentos em marketing e tomar decis\u00f5es de produto baseadas em dados, transformando a organiza\u00e7\u00e3o de uma empresa reativa para uma proativa e orientada por dados. Os resultados financeiros validam a abordagem: a an\u00e1lise de LTV provou que leads da campanha <code>google_gestao_agil</code> geram um retorno 2,3 vezes (ou 130%) superior ao de outros canais, fornecendo uma recomenda\u00e7\u00e3o de investimento inequ\u00edvoca.</p>"},{"location":"#objetivos-estrategicos","title":"Objetivos Estrat\u00e9gicos","text":"<p>O objetivo principal era resolver a \"cegueira estrat\u00e9gica\" na aquisi\u00e7\u00e3o de clientes. As metas de neg\u00f3cio inclu\u00edam: * Prioriza\u00e7\u00e3o de Vendas: Permitir que a equipa de vendas priorize esfor\u00e7os nos leads de maior potencial  * Otimiza\u00e7\u00e3o de ROI: Conectar o investimento em marketing (CAC) com a receita gerada (LTV), fornecendo uma base matem\u00e1tica para a aloca\u00e7\u00e3o de or\u00e7amento. * Intelig\u00eancia de Produto: Identificar os pontos de maior atrito na jornada do utilizador para priorizar melhorias no produto.</p>"},{"location":"#objetivos-tecnicos","title":"Objetivos T\u00e9cnicos","text":"<p>Para alcan\u00e7ar os objetivos estrat\u00e9gicos, as seguintes metas t\u00e9cnicas foram estabelecidas: * Construir um pipeline de dados de ponta a ponta, desde a captura de eventos no navegador at\u00e9 o armazenamento persistente. * Implementar um modelo de Lead Scoring para prever a probabilidade de convers\u00e3o. * Desenvolver modelos de LTV (BG/NBD e Gamma-Gamma) para quantificar o valor financeiro por segmento de cliente. * Automatizar o processo de pontua\u00e7\u00e3o de novos leads num ambiente de produ\u00e7\u00e3o na nuvem.</p>"},{"location":"#justificativa-das-tecnologias","title":"Justificativa das Tecnologias","text":"<p>A arquitetura foi constru\u00edda sobre uma pilha de tecnologias moderna, escal\u00e1vel e de c\u00f3digo aberto. * FastAPI: Escolhido para a API de ingest\u00e3o devido \u00e0 sua alta performance ass\u00edncrona e valida\u00e7\u00e3o de dados nativa com Pydantic. * PostgreSQL (via Supabase): Selecionado como um banco de dados relacional robusto e gerenciado na nuvem. * Render: Utilizado para a implanta\u00e7\u00e3o da API e do Cron Job de automa\u00e7\u00e3o. * Metabase (via Docker): Implementado como a ferramenta de BI para visualiza\u00e7\u00e3o, por ser uma solu\u00e7\u00e3o de c\u00f3digo aberto e eficiente para rodar localmente. * Scikit-learn / Lifetimes: Bibliotecas padr\u00e3o da ind\u00fastria para modelagem de machine learning e LTV, respetivamente.</p>"},{"location":"01_arquitetura/","title":"2. Arquitetura do Sistema","text":""},{"location":"01_arquitetura/#21-pipeline-de-dados","title":"2.1 Pipeline de Dados","text":"<p>O sistema \u00e9 um pipeline de dados de ponta a ponta que transforma cliques an\u00f3nimos em intelig\u00eancia de neg\u00f3cio. O fluxo \u00e9 o seguinte:</p> <ol> <li>Coleta (Navegador): O <code>tracker.js</code> no site captura eventos (ex: <code>page_view</code>) e dados de atribui\u00e7\u00e3o (UTMs).Ele gera um <code>visitor_id</code> \u00fanico e armazena dados de first/last touch no <code>localStorage</code>.</li> <li>Ingest\u00e3o (API FastAPI): Os dados s\u00e3o enviados via <code>POST</code> para a API (main.py) hospedada no Render.</li> <li>Valida\u00e7\u00e3o: A API usa modelos Pydantic para validar rigorosamente se o payload do evento corresponde ao schema definido, rejeitando dados malformados.</li> <li>Armazenamento (PostgreSQL): Dados validados s\u00e3o inseridos de forma segura no banco de dados PostgreSQL (Supabase).</li> <li>An\u00e1lise e Modelagem (Python/Jupyter): Scripts Python e notebooks conectam-se ao banco de dados para realizar engenharia de atributos (feature engineering) e treinar os modelos de ML (Lead Scoring e LTV).</li> <li>Automa\u00e7\u00e3o (Cron Job): Um script <code>score_updater.py</code> \u00e9 executado diariamente pelo Render. Ele carrega o modelo treinado, busca novos utilizadores, calcula seus scores e atualiza o banco de dados.</li> <li>Visualiza\u00e7\u00e3o (Metabase): O Metabase, rodando localmente via Docker, conecta-se ao banco de dados (em modo de leitura) para criar dashboards e relat\u00f3rios.</li> </ol>"},{"location":"01_arquitetura/#22-tecnologias-utilizadas","title":"2.2 Tecnologias Utilizadas","text":"Categoria Tecnologia Justificativa (Conforme Documenta\u00e7\u00e3o de Planejamento) Coleta JavaScript (custom) Script <code>tracker.js</code> para captura de eventos de front-end e atribui\u00e7\u00e3o Ingest\u00e3o (API) Python / FastAPI API ass\u00edncrona para valida\u00e7\u00e3o (Pydantic) e persist\u00eancia de dados. Armazenamento PostgreSQL (Supabase) Banco de dados relacional gerenciado na nuvem, servindo como reposit\u00f3rio central. An\u00e1lise/Modelagem Python / Jupyter Scripts e Notebooks para engenharia de atributos e treinamento de modelos. Automa\u00e7\u00e3o Cron Job (Render) Executa o script <code>score_updater.py</code> diariamente para pontuar novos leads. Visualiza\u00e7\u00e3o Metabase (via Docker) Ferramenta de BI open-source rodando localmente para an\u00e1lise visual."},{"location":"01_arquitetura/#23-diagrama-do-sistema","title":"2.3 Diagrama do Sistema","text":"<p>[cite_start]O diagrama a seguir ilustra o fluxo de dados completo do sistema InsightOS.</p> <p>Figura 1 - Diagrama de Fluxo de Dados do Sistema InsightOS</p> <p></p>"},{"location":"02_dados_sinteticos/","title":"3. Gera\u00e7\u00e3o de Dados Sint\u00e9ticos","text":""},{"location":"02_dados_sinteticos/#31-justificativa","title":"3.1 Justificativa","text":"<p>A gera\u00e7\u00e3o de dados sint\u00e9ticos foi uma decis\u00e3o estrat\u00e9gica na fase inicial do projeto. A principal justificativa foi a necessidade de um ambiente de laborat\u00f3rio controlado. Em vez de lidar com dados do mundo real, que poderiam ser escassos ou \"ruidosos\", a simula\u00e7\u00e3o nos permitiu: 1.  Validar o Pipeline: Garantir que o pipeline de ponta a ponta (coleta, API, BD, BI) funcionava corretamente. 2.  Criar Padr\u00f5es Fortes: Injetar intencionalmente padr\u00f5es e correla\u00e7\u00f5es realistas nos dados (ex: <code>google_gestao_agil</code> convertendo mais). 3.  Testar Modelos: Criar um \"quebra-cabe\u00e7as\" com uma solu\u00e7\u00e3o conhecida para validar se nossos modelos de ML e LTV eram capazes de \"encontrar\" os padr\u00f5es que criamos.</p>"},{"location":"02_dados_sinteticos/#32-implementacao","title":"3.2 Implementa\u00e7\u00e3o","text":"<p>O processo foi implementado no notebook <code>01_data_simulation.ipynb</code> , utilizando <code>pandas</code> para a cria\u00e7\u00e3o e manipula\u00e7\u00e3o dos DataFrames  e <code>faker</code> para gerar dados falsos com apar\u00eancia real (nomes, emails).</p> <p>1. Simula\u00e7\u00e3o do Funil de Convers\u00e3o (Lead Scoring): A estrat\u00e9gia central foi um funil probabil\u00edstico. Em vez de eventos aleat\u00f3rios, a ocorr\u00eancia de eventos-chave era uma cascata de decis\u00f5es com probabilidades condicionais.</p> <p>L\u00f3gica de Transi\u00e7\u00e3o (Exemplo <code>project_created</code>):  * Um utilizador s\u00f3 poderia criar um projeto se j\u00e1 tivesse verificado o e-mail.  * A probabilidade de criar um projeto era condicional \u00e0 campanha de origem.  * Isso foi implementado para simular que certas campanhas atraem utilizadores mais qualificados. O c\u00f3digo-fonte abaixo ilustra essa l\u00f3gica: ```python</p>"},{"location":"02_dados_sinteticos/#logica-da-simulacao-do-funil","title":"L\u00f3gica da simula\u00e7\u00e3o do funil","text":""},{"location":"02_dados_sinteticos/#a-probabilidade-de-criar-um-projeto-depende-da-campanha","title":"A probabilidade de criar um projeto depende da campanha","text":"<p>prob_create_project = 0.55 if user['acquisition_campaign'] == 'google_gestao_agil' else 0.30</p>"},{"location":"02_dados_sinteticos/#o-evento-so-ocorre-se-a-condicao-probabilistica-for-satisfeita","title":"O evento s\u00f3 ocorre se a condi\u00e7\u00e3o probabil\u00edstica for satisfeita","text":"<p>if random.random() &lt; prob_create_project:     # Gera o evento 'project_created'     events.append(...) </p> <pre><code># L\u00f3gica aninhada para a pr\u00f3xima etapa do funil (assinatura)\nprob_subscribe = 0.40 if user['acquisition_campaign'] == 'google_gestao_agil' else 0.15\nif random.random() &lt; prob_subscribe:\n    events.append(...)\n    # Atualiza o status do utilizador no DataFrame original\n    users_df.loc[users_df['user_id'] == user_id, 'status'] = 'converted'\n</code></pre> <p>2. Simula\u00e7\u00e3o de Transa\u00e7\u00f5es (LTV): Para a an\u00e1lise de LTV, um segundo script gerou um log de transa\u00e7\u00f5es. De forma similar, os dados foram intencionalmente enviesados para que clientes da <code>google_gestao_agil</code> tivessem: * Maior probabilidade de recompra. * Ciclo de compra mais curto. *Ticket m\u00e9dio (valor monet\u00e1rio) superior.</p> <p>3. Avalia\u00e7\u00e3o da Qualidade e Desafios: A \"qualidade\" dos dados foi validada pela capacidade de inseri-los e analis\u00e1-los. Durante a inser\u00e7\u00e3o, desafios t\u00e9cnicos reais foram superados: * <code>duplicate key value... \"users_pkey\"</code>: Resolvido limpando as tabelas antes de cada inser\u00e7\u00e3o para garantir um ambiente de teste limpo  * <code>duplicate key value... \"users_email_key\"</code>: Resolvido adicionando uma l\u00f3gica de verifica\u00e7\u00e3o para garantir que a biblioteca <code>faker</code> n\u00e3o gerasse e-mails duplicados . * <code>Out of range float values...</code>: Resolvido convertendo valores <code>np.nan</code> (do pandas) para <code>None</code> (Python nativo), que \u00e9 compat\u00edvel com o formato JSON <code>null</code> do banco de dados * A valida\u00e7\u00e3o final foi visual, usando o Metabase (Figura 2) para gerar um gr\u00e1fico de funil, que confirmou que a l\u00f3gica probabil\u00edstica foi capturada e armazenada corretamente.</p>"},{"location":"03_lead_scoring/","title":"4. Modelagem de Lead Scoring","text":""},{"location":"03_lead_scoring/#41-estrategia","title":"4.1 Estrat\u00e9gia","text":"<p>O objetivo era prever a vari\u00e1vel-alvo <code>target_converted</code> (1 para convertido, 0 para n\u00e3o convertido). A estrat\u00e9gia de modelagem foi:</p> <ol> <li>Engenharia de Atributos: Transformar os logs de eventos brutos (<code>user_events</code>) numa <code>feature_table</code> est\u00e1tica, onde cada linha \u00e9 um utilizador e cada coluna uma caracter\u00edstica (ex: contagem de <code>project_created</code>).</li> <li>Tratamento de Desequil\u00edbrio: A EDA revelou que apenas 10.1% dos utilizadores eram \"convertidos\". A t\u00e9cnica <code>class_weight='balanced'</code> foi usada no modelo para dar mais import\u00e2ncia \u00e0 classe minorit\u00e1ria e evitar que o modelo a ignorasse.</li> <li>Experimenta\u00e7\u00e3o Comparativa: Treinar um modelo baseline simples e interpret\u00e1vel (Regress\u00e3o Log\u00edstica) e compar\u00e1-lo com um modelo complexo e de alta performance (XGBoost) .</li> <li>Decis\u00e3o Baseada em Parcim\u00f4nia: Como ambos os modelos atingiram 100% de performance nos dados sint\u00e9ticos, o Princ\u00edpio da Parcim\u00f4nia foi aplicado: a solu\u00e7\u00e3o mais simples (Regress\u00e3o Log\u00edstica) foi escolhida por ser mais interpret\u00e1vel, eficiente e menos propensa a overfitting.</li> </ol>"},{"location":"03_lead_scoring/#42-detalhes-tecnicos","title":"4.2 Detalhes T\u00e9cnicos","text":"<p>O processo de modelagem foi encapsulado num <code>Pipeline</code> do Scikit-learn, garantindo que o pr\u00e9-processamento fosse aplicado de forma consistente.</p> <p>1. Pipeline de Pr\u00e9-processamento: Um <code>ColumnTransformer</code> foi usado para aplicar diferentes transforma\u00e7\u00f5es a diferentes colunas: * Colunas Num\u00e9ricas (ex: <code>events_project_created</code>): Aplicado <code>StandardScaler()</code> para padronizar os dados (coloc\u00e1-los na mesma escala). * Colunas Categ\u00f3ricas (ex: <code>campaign</code>): Aplicado <code>OneHotEncoder()</code> para transformar texto em colunas num\u00e9ricas (0/1). O par\u00e2metro <code>handle_unknown='ignore'</code> foi usado para tornar o modelo robusto a novas campanhas que possam surgir no futuro.</p> <p>2. C\u00f3digo do Pipeline (Estrutura): A estrutura final do pipeline, que combina pr\u00e9-processamento e o modelo, \u00e9 mostrada abaixo. ```python from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.linear_model import LogisticRegression</p>"},{"location":"03_lead_scoring/#definir-as-colunas-para-cada-transformador","title":"Definir as colunas para cada transformador","text":"<p>numeric_features = ['events_project_created', 'events_subscription_started', ...] categorical_features = ['campaign', 'plan', ...]</p>"},{"location":"03_lead_scoring/#criar-o-pre-processador","title":"Criar o pr\u00e9-processador","text":"<p>preprocessor = ColumnTransformer(     transformers=[         ('num', StandardScaler(), numeric_features),         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)     ])</p>"},{"location":"03_lead_scoring/#criar-o-pipeline-final-v1-regressao-logistica","title":"Criar o pipeline final v1 (Regress\u00e3o Log\u00edstica)","text":"<p>lr_pipeline = Pipeline(steps=[     ('preprocessor', preprocessor),     ('classifier', LogisticRegression(class_weight='balanced', random_state=42)) ])</p>"},{"location":"03_lead_scoring/#treinar-o-modelo","title":"Treinar o modelo","text":"<p>lr_pipeline.fit(X_train, y_train)</p>"},{"location":"03_lead_scoring/#salvar-o-pipeline-inteiro-para-producao","title":"Salvar o pipeline inteiro para produ\u00e7\u00e3o","text":"<p>import joblib joblib.dump(lr_pipeline, 'lead_scoring_pipeline_v1.joblib')</p> <p>3. M\u00e9tricas de Avalia\u00e7\u00e3o: A avalia\u00e7\u00e3o robusta (teste \u00fanico e Valida\u00e7\u00e3o Cruzada) confirmou a performance perfeita do pipeline nos dados simulados: * AUC: 1.0000  * Precis\u00e3o: 1.0000  * Recall (Sensibilidade): 1.0000  * Desvio Padr\u00e3o (Valida\u00e7\u00e3o Cruzada): 0.0000, indicando estabilidade total.</p> <p>Essa performance perfeita validou que o pipeline t\u00e9cnico era 100% funcional e capaz de \"resolver\" o problema \"f\u00e1cil\" que os dados sint\u00e9ticos apresentavam.</p> <p>O log abaixo (do ficheiro <code>modelo v2.png</code>) mostra a performance robusta do Modelo v2 (XGBoost), que tamb\u00e9m alcan\u00e7ou a perfei\u00e7\u00e3o:</p> <p></p> <p>A tabela de compara\u00e7\u00e3o final abaixo (do ficheiro <code>Celula 11...png</code>) resume a 'batalha' dos modelos e prova que ambas as performances foram id\u00eanticas, validando a nossa decis\u00e3o pelo \"Princ\u00edpio da Parcim\u00f4nia\" (escolher o Modelo v1, mais simples):</p> <p></p>"},{"location":"03_lead_scoring/#43-analise-visual-de-resultados-e-insights","title":"4.3 An\u00e1lise Visual de Resultados e Insights","text":"<p>1. Funil de Convers\u00e3o (Contexto EDA) O gr\u00e1fico de funil abaixo, gerado durante a An\u00e1lise Explorat\u00f3ria de Dados (EDA), foi crucial para contextualizar o problema. Ele identificou o maior ponto de atrito na jornada do cliente: * Ponto Cr\u00edtico: A maior quebra ocorre entre as etapas <code>user_verified</code> e <code>project_created</code>, onde menos da metade (41.5%) dos utilizadores engajados avan\u00e7a. * Implica\u00e7\u00e3o: Esta etapa (<code>project_created</code>) \u00e9 um dos preditores mais fortes para o modelo de scoring.</p> <p></p> <p>2. Distribui\u00e7\u00e3o dos Scores do Modelo Este gr\u00e1fico mostra o resultado final do nosso modelo (Regress\u00e3o Log\u00edstica v1) aplicado a todos os 1.000 utilizadores. * Descoberta: O resultado \u00e9 uma distribui\u00e7\u00e3o marcadamente bimodal, indicando que o modelo \u00e9 extremamente decisivo. * Grupo Frio (Score \u2248 0.0): A grande maioria dos leads (cerca de 90%) foi classificada com confian\u00e7a como \"fria\". * Grupo Quente (Score \u2248 1.0): Um segundo grupo claro (os restantes 10%) foi classificado com confian\u00e7a como \"quente\".  * Implica\u00e7\u00e3o: A aus\u00eancia de scores interm\u00e9dios (ex: 0.4, 0.5) \u00e9 uma vit\u00f3ria para o neg\u00f3cio, pois elimina a ambiguidade e permite \u00e0 equipa de vendas focar-se apenas no grupo \"quente\".</p> <p></p> <p>3. Insight de Neg\u00f3cio: Qualidade M\u00e9dia por Campanha</p> <p>Finalmente, este gr\u00e1fico conecta o resultado do modelo (o <code>lead_score</code>) de volta ao problema de neg\u00f3cio (investimento de marketing). * Descoberta: A campanha <code>google_gestao_agil</code> (score m\u00e9dio de 0.245) atrai leads de qualidade esmagadoramente superior a todas as outras fontes  * Implica\u00e7\u00e3o de ROI: Esta \u00e9 a principal recomenda\u00e7\u00e3o para a equipa de Marketing. O gr\u00e1fico prova que o or\u00e7amento deve ser realocado para esta campanha, pois ela gera os leads com maior probabilidade de convers\u00e3o, otimizando o ROI .</p> <p></p>"},{"location":"04_analise_ltv/","title":"5. An\u00e1lise Financeira (LTV)","text":""},{"location":"04_analise_ltv/#51-estrategia","title":"5.1 Estrat\u00e9gia","text":"<p>O objetivo desta fase era quantificar financeiramente o valor dos leads e provar o valor das campanhas. A estrat\u00e9gia foi usar modelos probabil\u00edsticos padr\u00e3o da ind\u00fastria: 1.  Modelo BG/NBD (Beta-Geometric/Negative-Binomial-Distribution): Usado para prever a atividade futura do cliente. Ele responde \"quantas compras futuras esperamos?\" e \"qual a probabilidade de o cliente estar 'vivo' (n\u00e3o ter sofrido churn)?\". 2.  Modelo Gamma-Gamma: Usado para estimar o valor monet\u00e1rio m\u00e9dio das transa\u00e7\u00f5es futuras. Ele s\u00f3 \u00e9 aplicado aos clientes que se espera estarem \"vivos\". 3.  C\u00e1lculo do LTV: O LTV \u00e9 calculado combinando as sa\u00eddas dos dois modelos (Frequ\u00eancia Esperada x Valor Monet\u00e1rio Esperado).</p>"},{"location":"04_analise_ltv/#52-descoberta-critica-violacao-de-premissa","title":"5.2 Descoberta Cr\u00edtica (Viola\u00e7\u00e3o de Premissa)","text":"<p>Antes de treinar o modelo Gamma-Gamma, uma verifica\u00e7\u00e3o de premissas \u00e9 necess\u00e1ria. O modelo assume que n\u00e3o h\u00e1 correla\u00e7\u00e3o entre a frequ\u00eancia de compra de um cliente e seu valor monet\u00e1rio m\u00e9dio. * Nossa An\u00e1lise: Encontramos uma correla\u00e7\u00e3o de 0.3882 (ou 0.39), como ser\u00e1 provado na imagem de log abaixo. * Tradu\u00e7\u00e3o de Neg\u00f3cio: Isso significa que \"clientes mais leais s\u00e3o tamb\u00e9m os que gastam mais\" \u2014 um insight valioso, mas que viola a premissa do modelo. * Decis\u00e3o Estrat\u00e9gica: A decis\u00e3o pragm\u00e1tica foi prosseguir. Em vez de tratar o LTV como uma previs\u00e3o cont\u00e1bil exata, n\u00f3s o interpretamos como um \u00edndice comparativo robusto para ranquear o valor relativo das campanhas.</p>"},{"location":"04_analise_ltv/#53-sumarios-de-performance-dos-modelos","title":"5.3 Sum\u00e1rios de Performance dos Modelos","text":"<p>1. Sum\u00e1rio do Modelo BG/NBD (Frequ\u00eancia e Churn) Este \u00e9 o log do primeiro modelo, que diagnosticou o comportamento \"m\u00e9dio\" de toda a base de clientes (quantas vezes compram e quando sofrem churn).</p> <p></p> <p>2. Sum\u00e1rio do Modelo Gamma-Gamma (Valor Monet\u00e1rio) Este log prova a nossa \"Descoberta Cr\u00edtica\" do passo anterior. A primeira linha confirma a Correla\u00e7\u00e3o de 0.3882 que viola a premissa. As linhas seguintes mostram o sum\u00e1rio do modelo de valor monet\u00e1rio.</p> <p></p>"},{"location":"04_analise_ltv/#54-visualizacao-preditiva-heatmap","title":"5.4 Visualiza\u00e7\u00e3o Preditiva (Heatmap)","text":"<p>O modelo BG/NBD tamb\u00e9m nos permite visualizar a \"sa\u00fade\" esperada de toda a base de clientes. Este heatmap (mapa de calor) mostra o n\u00famero esperado de compras futuras no pr\u00f3ximo per\u00edodo: * Eixo X (Frequ\u00eancia Hist\u00f3rica): Clientes que compraram mais vezes. * Eixo Y (Rec\u00eancia): Clientes que compraram mais recentemente (mais abaixo no gr\u00e1fico). * Insight (Mancha Clara): O canto inferior direito (clientes recentes e frequentes) \u00e9 o segmento com maior probabilidade de comprar novamente. O modelo confirma que os nossos melhores clientes s\u00e3o os que s\u00e3o simultaneamente recentes e frequentes.</p>"},{"location":"04_analise_ltv/#55-resultados-preditivos-top-10-clientes","title":"5.5 Resultados Preditivos (Top 10 Clientes)","text":"<p>O c\u00e1lculo do LTV combinado (BG/NBD + Gamma-Gamma) valida a nossa hip\u00f3tese de forma inequ\u00edvoca. A tabela abaixo, mostrando os 10 melhores clientes classificados pelo LTV futuro estimado, prova que o modelo conseguiu encontrar os padr\u00f5es que inserimos nos dados sint\u00e9ticos.</p> <ul> <li>Descoberta: Todos os 10 clientes com o maior LTV previsto pertencem, sem exce\u00e7\u00e3o, \u00e0 campanha <code>google_gestao_agil</code>.</li> <li>Valida\u00e7\u00e3o: Isto confirma que a nossa metodologia (os modelos BG/NBD e Gamma-Gamma) funciona. Eles foram capazes de identificar corretamente os clientes de alto valor que n\u00f3s cri\u00e1mos intencionalmente na simula\u00e7\u00e3o.</li> </ul> <p></p>"},{"location":"04_analise_ltv/#56-conclusao-financeira-agregacao-por-campanha","title":"5.6 Conclus\u00e3o Financeira (Agrega\u00e7\u00e3o por Campanha)","text":"<p>Finalmente, agregamos o LTV m\u00e9dio por campanha. O resultado abaixo \u00e9 a conclus\u00e3o financeira de todo o projeto, quantificando o que o modelo de lead scoring sugeriu.</p> <ul> <li>Prova Financeira: O LTV de um cliente da <code>google_gestao_agil</code> \u00e9 de R$73.60.</li> <li>ROI Comparativo: Este valor \u00e9 2.3 vezes maior (ou 130% superior) ao LTV m\u00e9dio das outras campanhas.</li> <li>Recomenda\u00e7\u00e3o Estrat\u00e9gica: Esta an\u00e1lise fornece a prova matem\u00e1tica inequ\u00edvoca para a realoca\u00e7\u00e3o agressiva do or\u00e7amento de marketing para a campanha <code>google_gestao_agil</code>.</li> </ul> <p></p>"},{"location":"05_automacao/","title":"6. Automa\u00e7\u00e3o (Produ\u00e7\u00e3o)","text":""},{"location":"05_automacao/#61-scripts-automatizados","title":"6.1 Scripts Automatizados","text":"<p>A etapa final foi operacionalizar o modelo de Lead Scoring, transformando-o de uma an\u00e1lise est\u00e1tica num sistema de neg\u00f3cio aut\u00f3nomo.</p> <ul> <li>Script: <code>score_updater.py</code>.</li> <li>Plataforma: Cron Job no Render.</li> <li>Tarefa: Executar diariamente, conectar-se ao BD, buscar utilizadores sem <code>lead_score</code>, carregar o pipeline (<code>lead_scoring_pipeline_v1.joblib</code>), gerar as previs\u00f5es e atualizar a coluna <code>lead_score</code> na tabela <code>users</code>.</li> </ul> <p>Desafio de Produ\u00e7\u00e3o e Solu\u00e7\u00e3o (Processamento em Lotes): Durante o teste local do script de produ\u00e7\u00e3o, foi encontrado um erro <code>414 Request-URI Too Large</code>. * Causa: O script tentava buscar os eventos de todos os utilizadores (1000) numa \u00fanica requisi\u00e7\u00e3o \u00e0 API, criando uma URL longa demais para o servidor. * Solu\u00e7\u00e3o: Foi implementada uma l\u00f3gica de processamento em lotes (batch processing). O script foi modificado para processar os utilizadores em \"fatias\" de 100.</p> <p>O c\u00f3digo-fonte abaixo ilustra esta l\u00f3gica de produ\u00e7\u00e3o robusta: ```python</p>"},{"location":"05_automacao/#definir-o-tamanho-do-lote","title":"Definir o tamanho do lote","text":"<p>BATCH_SIZE = 100 users_processed = 0</p>"},{"location":"05_automacao/#obter-o-total-de-utilizadores-para-pontuar","title":"Obter o total de utilizadores para pontuar","text":"<p>users_to_score = supabase.table('users').select('user_id', count='exact').execute().count</p> <p>while users_processed &lt; users_to_score:     print(f\"--- Processando Lote (utilizadores {users_processed} a {users_processed + BATCH_SIZE}) ---\")</p> <pre><code># 1. Buscar um \"lote\" de utilizadores\nuser_batch = supabase.table('users').select('user_id', 'visitor_id') \\\n                       .is_('lead_score', 'null') \\\n                       .range(users_processed, users_processed + BATCH_SIZE - 1) \\\n                       .execute().data\n\nif not user_batch:\n    break # Sai do loop se n\u00e3o houver mais utilizadores\n\n# ... (L\u00f3gica para buscar eventos apenas para este lote) ...\n\n# ... (Construir a feature_table apenas para este lote) ...\n\n# 2. Calcular scores para o lote\nscores = pipeline.predict_proba(feature_table)[:, 1]\n\n# 3. Atualizar o banco de dados para o lote\nupdates = []\nfor user, score in zip(user_batch, scores):\n    updates.append({'user_id': user['user_id'], 'lead_score': score})\n\nsupabase.table('users').upsert(updates).execute()\n\nusers_processed += len(user_batch)\n</code></pre> <p>print(\"Atualiza\u00e7\u00e3o de Lead Score conclu\u00edda.\")</p>"},{"location":"05_automacao/#prova-de-execucao-local-do-script","title":"Prova de Execu\u00e7\u00e3o Local do Script","text":"<p>As imagens abaixo demonstram o script <code>score_updater.py</code> a ser executado localmente, validando a l\u00f3gica de processamento em lotes. O script processa com sucesso os 1.000 utilizadores em 10 lotes separados de 100, evitando sobrecarga do servidor.</p> <p>In\u00edcio do Processo (Lote 1 de 10): </p> <p>Meio do Processo (Lote 5 de 10): </p> <p>Fim do Processo (Lote 10 de 10): </p>"},{"location":"05_automacao/#monitoramento-e-desafios-de-implantacao","title":"Monitoramento e Desafios de Implanta\u00e7\u00e3o:","text":"<p>O deploy no Render foi monitorado atrav\u00e9s do dashboard de \"Events\". Como \u00e9 comum em qualquer processo de deploy do mundo real, o processo revelou bugs de produ\u00e7\u00e3o que foram diagnosticados e corrigidos metodicamente.</p> <p>O log de eventos abaixo ilustra perfeitamente este processo de depura\u00e7\u00e3o:</p> <p></p> <p>A an\u00e1lise do log acima mostra a resolu\u00e7\u00e3o de dois bugs cl\u00e1ssicos de produ\u00e7\u00e3o:</p> <ol> <li>Erro (07:37:35 PM): <code>ModuleNotFoundError: No module named 'pandas'</code><ul> <li>Causa: O <code>requirements.txt</code> enviado para o Render n\u00e3o continha as bibliotecas <code>pandas</code> e <code>numpy</code>.</li> <li>Solu\u00e7\u00e3o: As depend\u00eancias foram adicionadas ao <code>requirements.txt</code> e um novo commit foi enviado.</li> </ul> </li> <li>Erro (07:47:28 PM): <code>Invalid API key</code><ul> <li>Causa: Ap\u00f3s corrigir o primeiro bug, o script foi executado, mas falhou ao conectar-se ao Supabase. As credenciais (<code>SUPABASE_KEY</code>) nas Vari\u00e1veis de Ambiente do Render estavam incorretas.</li> <li>Solu\u00e7\u00e3o: As credenciais foram corrigidas diretamente no dashboard do Render.</li> </ul> </li> </ol> <p>Ap\u00f3s as corre\u00e7\u00f5es, o \"build\" foi bem-sucedido, como mostra o dashboard do Render:</p> <p></p> <p>O Cron Job final (executado \u00e0s 08:18:56 PM no primeiro log) foi executado com sucesso (\"Process completed\"), validando que o sistema automatizado estava 100% funcional. A imagem abaixo, do banco de dados Supabase, mostra o resultado final: a coluna <code>lead_score</code> corretamente preenchida pelo script automatizado.</p> <p></p>"}]}